{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8433dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b1bbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Element</th>\n",
       "      <th>M-At No.</th>\n",
       "      <th>M-At wt.</th>\n",
       "      <th>M-Density</th>\n",
       "      <th>M-M.P</th>\n",
       "      <th>M-B.P</th>\n",
       "      <th>M-Enth.fus</th>\n",
       "      <th>M-Enth.atom</th>\n",
       "      <th>M-Enth.vap</th>\n",
       "      <th>M-Sp.ht Cap</th>\n",
       "      <th>...</th>\n",
       "      <th>Elec.-ve</th>\n",
       "      <th>Surface.E</th>\n",
       "      <th>1st Ion E</th>\n",
       "      <th>cova .radii</th>\n",
       "      <th>At.radii</th>\n",
       "      <th>Group</th>\n",
       "      <th>Period</th>\n",
       "      <th>Work F.</th>\n",
       "      <th>Elec.Aff</th>\n",
       "      <th>OH_B.E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Au3M-Ni</td>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.92</td>\n",
       "      <td>737.1</td>\n",
       "      <td>121</td>\n",
       "      <td>149</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.24</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1.496072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Au3M-Zn</td>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.33</td>\n",
       "      <td>906.4</td>\n",
       "      <td>122</td>\n",
       "      <td>142</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.63</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>1.398238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Au3M-Ru</td>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.60</td>\n",
       "      <td>710.2</td>\n",
       "      <td>126</td>\n",
       "      <td>178</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.65</td>\n",
       "      <td>101.3</td>\n",
       "      <td>1.372716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Au3M-Rh</td>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.98</td>\n",
       "      <td>719.7</td>\n",
       "      <td>135</td>\n",
       "      <td>173</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.20</td>\n",
       "      <td>109.7</td>\n",
       "      <td>1.527600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Au3M-Pd</td>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.36</td>\n",
       "      <td>804.4</td>\n",
       "      <td>131</td>\n",
       "      <td>169</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.47</td>\n",
       "      <td>53.7</td>\n",
       "      <td>1.484960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>Re-Re3M-Ta</td>\n",
       "      <td>75</td>\n",
       "      <td>186.21</td>\n",
       "      <td>21.02</td>\n",
       "      <td>3459.00</td>\n",
       "      <td>5869</td>\n",
       "      <td>33.0</td>\n",
       "      <td>776</td>\n",
       "      <td>705</td>\n",
       "      <td>137.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.34</td>\n",
       "      <td>761.0</td>\n",
       "      <td>170</td>\n",
       "      <td>200</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.00</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-0.240743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>Re-Re3M-W</td>\n",
       "      <td>75</td>\n",
       "      <td>186.21</td>\n",
       "      <td>21.02</td>\n",
       "      <td>3459.00</td>\n",
       "      <td>5869</td>\n",
       "      <td>33.0</td>\n",
       "      <td>776</td>\n",
       "      <td>705</td>\n",
       "      <td>137.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.36</td>\n",
       "      <td>3.23</td>\n",
       "      <td>770.0</td>\n",
       "      <td>162</td>\n",
       "      <td>193</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.45</td>\n",
       "      <td>78.6</td>\n",
       "      <td>-0.193473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Re-Re3M-Os</td>\n",
       "      <td>75</td>\n",
       "      <td>186.21</td>\n",
       "      <td>21.02</td>\n",
       "      <td>3459.00</td>\n",
       "      <td>5869</td>\n",
       "      <td>33.0</td>\n",
       "      <td>776</td>\n",
       "      <td>705</td>\n",
       "      <td>137.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.95</td>\n",
       "      <td>840.0</td>\n",
       "      <td>128</td>\n",
       "      <td>185</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.93</td>\n",
       "      <td>106.1</td>\n",
       "      <td>0.087294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Re-Re3M-Ir</td>\n",
       "      <td>75</td>\n",
       "      <td>186.21</td>\n",
       "      <td>21.02</td>\n",
       "      <td>3459.00</td>\n",
       "      <td>5869</td>\n",
       "      <td>33.0</td>\n",
       "      <td>776</td>\n",
       "      <td>705</td>\n",
       "      <td>137.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.36</td>\n",
       "      <td>880.0</td>\n",
       "      <td>137</td>\n",
       "      <td>180</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.76</td>\n",
       "      <td>151.0</td>\n",
       "      <td>0.001003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Re-Re3M-Pt</td>\n",
       "      <td>75</td>\n",
       "      <td>186.21</td>\n",
       "      <td>21.02</td>\n",
       "      <td>3459.00</td>\n",
       "      <td>5869</td>\n",
       "      <td>33.0</td>\n",
       "      <td>776</td>\n",
       "      <td>705</td>\n",
       "      <td>137.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.49</td>\n",
       "      <td>870.0</td>\n",
       "      <td>136</td>\n",
       "      <td>177</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.97</td>\n",
       "      <td>205.3</td>\n",
       "      <td>-0.062437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Element  M-At No.  M-At wt.  M-Density    M-M.P  M-B.P  M-Enth.fus  \\\n",
       "0       Au3M-Ni        79    196.96      19.30  1337.33   3129        12.5   \n",
       "1       Au3M-Zn        79    196.96      19.30  1337.33   3129        12.5   \n",
       "2       Au3M-Ru        79    196.96      19.30  1337.33   3129        12.5   \n",
       "3       Au3M-Rh        79    196.96      19.30  1337.33   3129        12.5   \n",
       "4       Au3M-Pd        79    196.96      19.30  1337.33   3129        12.5   \n",
       "..          ...       ...       ...        ...      ...    ...         ...   \n",
       "337  Re-Re3M-Ta        75    186.21      21.02  3459.00   5869        33.0   \n",
       "338   Re-Re3M-W        75    186.21      21.02  3459.00   5869        33.0   \n",
       "339  Re-Re3M-Os        75    186.21      21.02  3459.00   5869        33.0   \n",
       "340  Re-Re3M-Ir        75    186.21      21.02  3459.00   5869        33.0   \n",
       "341  Re-Re3M-Pt        75    186.21      21.02  3459.00   5869        33.0   \n",
       "\n",
       "     M-Enth.atom  M-Enth.vap  M-Sp.ht Cap  ...  Elec.-ve  Surface.E  \\\n",
       "0            368         330        129.1  ...      1.91       1.92   \n",
       "1            368         330        129.1  ...      1.65       0.33   \n",
       "2            368         330        129.1  ...      2.20       2.60   \n",
       "3            368         330        129.1  ...      2.28       1.98   \n",
       "4            368         330        129.1  ...      2.20       1.36   \n",
       "..           ...         ...          ...  ...       ...        ...   \n",
       "337          776         705        137.0  ...      1.50       2.34   \n",
       "338          776         705        137.0  ...      2.36       3.23   \n",
       "339          776         705        137.0  ...      2.20       2.95   \n",
       "340          776         705        137.0  ...      2.20       2.36   \n",
       "341          776         705        137.0  ...      2.28       1.49   \n",
       "\n",
       "     1st Ion E  cova .radii  At.radii  Group  Period  Work F.  Elec.Aff  \\\n",
       "0        737.1          121       149   10.0       4     5.24     112.0   \n",
       "1        906.4          122       142   12.0       4     3.63     -58.0   \n",
       "2        710.2          126       178    8.0       5     4.65     101.3   \n",
       "3        719.7          135       173    9.0       5     5.20     109.7   \n",
       "4        804.4          131       169   10.0       5     5.47      53.7   \n",
       "..         ...          ...       ...    ...     ...      ...       ...   \n",
       "337      761.0          170       200    5.0       6     4.00      31.0   \n",
       "338      770.0          162       193    6.0       6     4.45      78.6   \n",
       "339      840.0          128       185    8.0       6     5.93     106.1   \n",
       "340      880.0          137       180    9.0       6     5.76     151.0   \n",
       "341      870.0          136       177   10.0       6     5.97     205.3   \n",
       "\n",
       "       OH_B.E  \n",
       "0    1.496072  \n",
       "1    1.398238  \n",
       "2    1.372716  \n",
       "3    1.527600  \n",
       "4    1.484960  \n",
       "..        ...  \n",
       "337 -0.240743  \n",
       "338 -0.193473  \n",
       "339  0.087294  \n",
       "340  0.001003  \n",
       "341 -0.062437  \n",
       "\n",
       "[342 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('Dataset_A3B_OH_343.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "900236db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M-At No.</th>\n",
       "      <th>M-At wt.</th>\n",
       "      <th>M-Density</th>\n",
       "      <th>M-M.P</th>\n",
       "      <th>M-B.P</th>\n",
       "      <th>M-Enth.fus</th>\n",
       "      <th>M-Enth.atom</th>\n",
       "      <th>M-Enth.vap</th>\n",
       "      <th>M-Sp.ht Cap</th>\n",
       "      <th>M-Elec.-ve</th>\n",
       "      <th>...</th>\n",
       "      <th>Elec.-ve</th>\n",
       "      <th>Surface.E</th>\n",
       "      <th>1st Ion E</th>\n",
       "      <th>cova .radii</th>\n",
       "      <th>At.radii</th>\n",
       "      <th>Group</th>\n",
       "      <th>Period</th>\n",
       "      <th>Work F.</th>\n",
       "      <th>Elec.Aff</th>\n",
       "      <th>OH_B.E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.92</td>\n",
       "      <td>737.10</td>\n",
       "      <td>121</td>\n",
       "      <td>149</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.24</td>\n",
       "      <td>112.0</td>\n",
       "      <td>1.496072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.33</td>\n",
       "      <td>906.40</td>\n",
       "      <td>122</td>\n",
       "      <td>142</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.63</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>1.398238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.60</td>\n",
       "      <td>710.20</td>\n",
       "      <td>126</td>\n",
       "      <td>178</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.65</td>\n",
       "      <td>101.3</td>\n",
       "      <td>1.372716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.98</td>\n",
       "      <td>719.70</td>\n",
       "      <td>135</td>\n",
       "      <td>173</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.20</td>\n",
       "      <td>109.7</td>\n",
       "      <td>1.527600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.36</td>\n",
       "      <td>804.40</td>\n",
       "      <td>131</td>\n",
       "      <td>169</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.47</td>\n",
       "      <td>53.7</td>\n",
       "      <td>1.484960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>77</td>\n",
       "      <td>192.22</td>\n",
       "      <td>22.56</td>\n",
       "      <td>2739.00</td>\n",
       "      <td>4701</td>\n",
       "      <td>26.0</td>\n",
       "      <td>671</td>\n",
       "      <td>560</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.57</td>\n",
       "      <td>760.00</td>\n",
       "      <td>159</td>\n",
       "      <td>188</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.72</td>\n",
       "      <td>14.5</td>\n",
       "      <td>0.704786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>77</td>\n",
       "      <td>192.22</td>\n",
       "      <td>22.56</td>\n",
       "      <td>2739.00</td>\n",
       "      <td>4701</td>\n",
       "      <td>26.0</td>\n",
       "      <td>671</td>\n",
       "      <td>560</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.95</td>\n",
       "      <td>840.00</td>\n",
       "      <td>128</td>\n",
       "      <td>185</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.93</td>\n",
       "      <td>106.1</td>\n",
       "      <td>0.679329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>77</td>\n",
       "      <td>192.22</td>\n",
       "      <td>22.56</td>\n",
       "      <td>2739.00</td>\n",
       "      <td>4701</td>\n",
       "      <td>26.0</td>\n",
       "      <td>671</td>\n",
       "      <td>560</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.49</td>\n",
       "      <td>870.00</td>\n",
       "      <td>136</td>\n",
       "      <td>177</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.97</td>\n",
       "      <td>205.3</td>\n",
       "      <td>0.531434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>77</td>\n",
       "      <td>192.22</td>\n",
       "      <td>22.56</td>\n",
       "      <td>2739.00</td>\n",
       "      <td>4701</td>\n",
       "      <td>26.0</td>\n",
       "      <td>671</td>\n",
       "      <td>560</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0.74</td>\n",
       "      <td>890.13</td>\n",
       "      <td>136</td>\n",
       "      <td>174</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.41</td>\n",
       "      <td>222.8</td>\n",
       "      <td>0.523295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>77</td>\n",
       "      <td>192.22</td>\n",
       "      <td>22.56</td>\n",
       "      <td>2739.00</td>\n",
       "      <td>4701</td>\n",
       "      <td>26.0</td>\n",
       "      <td>671</td>\n",
       "      <td>560</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.17</td>\n",
       "      <td>703.00</td>\n",
       "      <td>148</td>\n",
       "      <td>143</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.34</td>\n",
       "      <td>91.2</td>\n",
       "      <td>0.291225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    M-At No.  M-At wt.  M-Density    M-M.P  M-B.P  M-Enth.fus  M-Enth.atom  \\\n",
       "0         79    196.96      19.30  1337.33   3129        12.5          368   \n",
       "1         79    196.96      19.30  1337.33   3129        12.5          368   \n",
       "2         79    196.96      19.30  1337.33   3129        12.5          368   \n",
       "3         79    196.96      19.30  1337.33   3129        12.5          368   \n",
       "4         79    196.96      19.30  1337.33   3129        12.5          368   \n",
       "..       ...       ...        ...      ...    ...         ...          ...   \n",
       "64        77    192.22      22.56  2739.00   4701        26.0          671   \n",
       "65        77    192.22      22.56  2739.00   4701        26.0          671   \n",
       "66        77    192.22      22.56  2739.00   4701        26.0          671   \n",
       "67        77    192.22      22.56  2739.00   4701        26.0          671   \n",
       "68        77    192.22      22.56  2739.00   4701        26.0          671   \n",
       "\n",
       "    M-Enth.vap  M-Sp.ht Cap  M-Elec.-ve  ...  Elec.-ve  Surface.E  1st Ion E  \\\n",
       "0          330        129.1        2.54  ...      1.91       1.92     737.10   \n",
       "1          330        129.1        2.54  ...      1.65       0.33     906.40   \n",
       "2          330        129.1        2.54  ...      2.20       2.60     710.20   \n",
       "3          330        129.1        2.54  ...      2.28       1.98     719.70   \n",
       "4          330        129.1        2.54  ...      2.20       1.36     804.40   \n",
       "..         ...          ...         ...  ...       ...        ...        ...   \n",
       "64         560        131.0        2.20  ...      1.90       2.57     760.00   \n",
       "65         560        131.0        2.20  ...      2.20       2.95     840.00   \n",
       "66         560        131.0        2.20  ...      2.28       1.49     870.00   \n",
       "67         560        131.0        2.20  ...      2.54       0.74     890.13   \n",
       "68         560        131.0        2.20  ...      2.02       0.17     703.00   \n",
       "\n",
       "    cova .radii  At.radii  Group  Period  Work F.  Elec.Aff    OH_B.E  \n",
       "0           121       149   10.0       4     5.24     112.0  1.496072  \n",
       "1           122       142   12.0       4     3.63     -58.0  1.398238  \n",
       "2           126       178    8.0       5     4.65     101.3  1.372716  \n",
       "3           135       173    9.0       5     5.20     109.7  1.527600  \n",
       "4           131       169   10.0       5     5.47      53.7  1.484960  \n",
       "..          ...       ...    ...     ...      ...       ...       ...  \n",
       "64          159       188    7.0       6     4.72      14.5  0.704786  \n",
       "65          128       185    8.0       6     5.93     106.1  0.679329  \n",
       "66          136       177   10.0       6     5.97     205.3  0.531434  \n",
       "67          136       174   11.0       6     5.41     222.8  0.523295  \n",
       "68          148       143   15.0       6     4.34      91.2  0.291225  \n",
       "\n",
       "[69 rows x 37 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df.iloc[0:69,1:38]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc4165f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M-At No.', 'M-At wt.', 'M-Density', 'M-M.P', 'M-B.P', 'M-Enth.fus', 'M-Enth.atom', 'M-Enth.vap', 'M-Sp.ht Cap', 'M-Elec.-ve', 'M-Surface.E', 'M-1st Ion E', 'M-cova .radii', 'M-At.radii', 'M-Group', 'M-Period', 'M-Work F.', 'M-Elec.Aff', 'At No.', 'At wt.', 'Density', 'M.P', 'B.P', 'Enth.fus', 'Enth.atom', 'Enth.vap', 'Sp.ht Cap', 'Elec.-ve', 'Surface.E', '1st Ion E', 'cova .radii', 'At.radii', 'Group', 'Period', 'Work F.', 'Elec.Aff', 'OH_B.E']\n"
     ]
    }
   ],
   "source": [
    "print(df2.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d45ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df2\n",
    "y= df2['OH_B.E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40d9d8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48, 37), (21, 37))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e17648",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b9dc1e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M-At No.</th>\n",
       "      <th>M-At wt.</th>\n",
       "      <th>M-Density</th>\n",
       "      <th>M-M.P</th>\n",
       "      <th>M-B.P</th>\n",
       "      <th>M-Enth.fus</th>\n",
       "      <th>M-Enth.atom</th>\n",
       "      <th>M-Enth.vap</th>\n",
       "      <th>M-Sp.ht Cap</th>\n",
       "      <th>M-Elec.-ve</th>\n",
       "      <th>...</th>\n",
       "      <th>Sp.ht Cap</th>\n",
       "      <th>Elec.-ve</th>\n",
       "      <th>Surface.E</th>\n",
       "      <th>1st Ion E</th>\n",
       "      <th>cova .radii</th>\n",
       "      <th>At.radii</th>\n",
       "      <th>Group</th>\n",
       "      <th>Period</th>\n",
       "      <th>Work F.</th>\n",
       "      <th>Elec.Aff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>445.0</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.92</td>\n",
       "      <td>737.10</td>\n",
       "      <td>121</td>\n",
       "      <td>149</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.24</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>388.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.33</td>\n",
       "      <td>906.40</td>\n",
       "      <td>122</td>\n",
       "      <td>142</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.63</td>\n",
       "      <td>-58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>238.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.60</td>\n",
       "      <td>710.20</td>\n",
       "      <td>126</td>\n",
       "      <td>178</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.65</td>\n",
       "      <td>101.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.98</td>\n",
       "      <td>719.70</td>\n",
       "      <td>135</td>\n",
       "      <td>173</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.20</td>\n",
       "      <td>109.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79</td>\n",
       "      <td>196.96</td>\n",
       "      <td>19.30</td>\n",
       "      <td>1337.33</td>\n",
       "      <td>3129</td>\n",
       "      <td>12.5</td>\n",
       "      <td>368</td>\n",
       "      <td>330</td>\n",
       "      <td>129.1</td>\n",
       "      <td>2.54</td>\n",
       "      <td>...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.36</td>\n",
       "      <td>804.40</td>\n",
       "      <td>131</td>\n",
       "      <td>169</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.47</td>\n",
       "      <td>53.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>77</td>\n",
       "      <td>192.22</td>\n",
       "      <td>22.56</td>\n",
       "      <td>2739.00</td>\n",
       "      <td>4701</td>\n",
       "      <td>26.0</td>\n",
       "      <td>671</td>\n",
       "      <td>560</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>137.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.57</td>\n",
       "      <td>760.00</td>\n",
       "      <td>159</td>\n",
       "      <td>188</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.72</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>77</td>\n",
       "      <td>192.22</td>\n",
       "      <td>22.56</td>\n",
       "      <td>2739.00</td>\n",
       "      <td>4701</td>\n",
       "      <td>26.0</td>\n",
       "      <td>671</td>\n",
       "      <td>560</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>130.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.95</td>\n",
       "      <td>840.00</td>\n",
       "      <td>128</td>\n",
       "      <td>185</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.93</td>\n",
       "      <td>106.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>77</td>\n",
       "      <td>192.22</td>\n",
       "      <td>22.56</td>\n",
       "      <td>2739.00</td>\n",
       "      <td>4701</td>\n",
       "      <td>26.0</td>\n",
       "      <td>671</td>\n",
       "      <td>560</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>133.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.49</td>\n",
       "      <td>870.00</td>\n",
       "      <td>136</td>\n",
       "      <td>177</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.97</td>\n",
       "      <td>205.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>77</td>\n",
       "      <td>192.22</td>\n",
       "      <td>22.56</td>\n",
       "      <td>2739.00</td>\n",
       "      <td>4701</td>\n",
       "      <td>26.0</td>\n",
       "      <td>671</td>\n",
       "      <td>560</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>129.1</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0.74</td>\n",
       "      <td>890.13</td>\n",
       "      <td>136</td>\n",
       "      <td>174</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.41</td>\n",
       "      <td>222.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>77</td>\n",
       "      <td>192.22</td>\n",
       "      <td>22.56</td>\n",
       "      <td>2739.00</td>\n",
       "      <td>4701</td>\n",
       "      <td>26.0</td>\n",
       "      <td>671</td>\n",
       "      <td>560</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>...</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2.02</td>\n",
       "      <td>0.17</td>\n",
       "      <td>703.00</td>\n",
       "      <td>148</td>\n",
       "      <td>143</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.34</td>\n",
       "      <td>91.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    M-At No.  M-At wt.  M-Density    M-M.P  M-B.P  M-Enth.fus  M-Enth.atom  \\\n",
       "0         79    196.96      19.30  1337.33   3129        12.5          368   \n",
       "1         79    196.96      19.30  1337.33   3129        12.5          368   \n",
       "2         79    196.96      19.30  1337.33   3129        12.5          368   \n",
       "3         79    196.96      19.30  1337.33   3129        12.5          368   \n",
       "4         79    196.96      19.30  1337.33   3129        12.5          368   \n",
       "..       ...       ...        ...      ...    ...         ...          ...   \n",
       "64        77    192.22      22.56  2739.00   4701        26.0          671   \n",
       "65        77    192.22      22.56  2739.00   4701        26.0          671   \n",
       "66        77    192.22      22.56  2739.00   4701        26.0          671   \n",
       "67        77    192.22      22.56  2739.00   4701        26.0          671   \n",
       "68        77    192.22      22.56  2739.00   4701        26.0          671   \n",
       "\n",
       "    M-Enth.vap  M-Sp.ht Cap  M-Elec.-ve  ...  Sp.ht Cap  Elec.-ve  Surface.E  \\\n",
       "0          330        129.1        2.54  ...      445.0      1.91       1.92   \n",
       "1          330        129.1        2.54  ...      388.0      1.65       0.33   \n",
       "2          330        129.1        2.54  ...      238.0      2.20       2.60   \n",
       "3          330        129.1        2.54  ...      240.0      2.28       1.98   \n",
       "4          330        129.1        2.54  ...      240.0      2.20       1.36   \n",
       "..         ...          ...         ...  ...        ...       ...        ...   \n",
       "64         560        131.0        2.20  ...      137.0      1.90       2.57   \n",
       "65         560        131.0        2.20  ...      130.0      2.20       2.95   \n",
       "66         560        131.0        2.20  ...      133.0      2.28       1.49   \n",
       "67         560        131.0        2.20  ...      129.1      2.54       0.74   \n",
       "68         560        131.0        2.20  ...      122.0      2.02       0.17   \n",
       "\n",
       "    1st Ion E  cova .radii  At.radii  Group  Period  Work F.  Elec.Aff  \n",
       "0      737.10          121       149   10.0       4     5.24     112.0  \n",
       "1      906.40          122       142   12.0       4     3.63     -58.0  \n",
       "2      710.20          126       178    8.0       5     4.65     101.3  \n",
       "3      719.70          135       173    9.0       5     5.20     109.7  \n",
       "4      804.40          131       169   10.0       5     5.47      53.7  \n",
       "..        ...          ...       ...    ...     ...      ...       ...  \n",
       "64     760.00          159       188    7.0       6     4.72      14.5  \n",
       "65     840.00          128       185    8.0       6     5.93     106.1  \n",
       "66     870.00          136       177   10.0       6     5.97     205.3  \n",
       "67     890.13          136       174   11.0       6     5.41     222.8  \n",
       "68     703.00          148       143   15.0       6     4.34      91.2  \n",
       "\n",
       "[69 rows x 36 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = df2.drop(labels=[\"OH_B.E\"], axis=1)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaef50f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "cor = df3.corr()\n",
    "G=sns.heatmap(cor,annot=True,cmap=\"RdYlBu\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cba48fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df3\n",
    "y = df2['OH_B.E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8183bc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48, 36), (21, 36))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9289beab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 on test data is 0.8242208684728907\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "print(\"r2 on test data is\",   r2_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85b6839b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:0.22906527244259625\n"
     ]
    }
   ],
   "source": [
    "print(\"RMSE:\"+str(np.sqrt(mean_squared_error(y_test, y_predict))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa493186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'reg:squarederror',\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': None,\n",
       " 'feature_types': None,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'grow_policy': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_bin': None,\n",
       " 'max_cat_threshold': None,\n",
       " 'max_cat_to_onehot': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'max_leaves': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'predictor': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'sampling_method': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H_params = model.get_params()\n",
    "H_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d44e8281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b72f3aca00>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test, y_predict)\n",
    "plt.plot([-2,2], [-2,2],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "476ea20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.read_csv('Cu_based_bi_OH_BE.csv')\n",
    "df_pred\n",
    "selected_columns = df_pred[[\"Element\"]]\n",
    "new_df = selected_columns.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeec5090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M-At No.</th>\n",
       "      <th>M-At wt.</th>\n",
       "      <th>M-Density</th>\n",
       "      <th>M-M.P</th>\n",
       "      <th>M-B.P</th>\n",
       "      <th>M-Enth.fus</th>\n",
       "      <th>M-Enth.atom</th>\n",
       "      <th>M-Enth.vap</th>\n",
       "      <th>M-Sp.ht Cap</th>\n",
       "      <th>M-Elec.-ve</th>\n",
       "      <th>...</th>\n",
       "      <th>Sp.ht Cap</th>\n",
       "      <th>Elec.-ve</th>\n",
       "      <th>Surface.E</th>\n",
       "      <th>1st Ion E</th>\n",
       "      <th>cova .radii</th>\n",
       "      <th>At.radii</th>\n",
       "      <th>Group</th>\n",
       "      <th>Period</th>\n",
       "      <th>Work F.</th>\n",
       "      <th>Elec.Aff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>567.0</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.20</td>\n",
       "      <td>633.10</td>\n",
       "      <td>170</td>\n",
       "      <td>184</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.500</td>\n",
       "      <td>18.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>520.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.93</td>\n",
       "      <td>658.80</td>\n",
       "      <td>160</td>\n",
       "      <td>176</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.330</td>\n",
       "      <td>7.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>489.0</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.38</td>\n",
       "      <td>650.90</td>\n",
       "      <td>153</td>\n",
       "      <td>171</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.300</td>\n",
       "      <td>50.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>448.0</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3.22</td>\n",
       "      <td>652.90</td>\n",
       "      <td>139</td>\n",
       "      <td>166</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4.500</td>\n",
       "      <td>64.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>479.0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>3.39</td>\n",
       "      <td>717.30</td>\n",
       "      <td>139</td>\n",
       "      <td>161</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4.100</td>\n",
       "      <td>-50.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>421.0</td>\n",
       "      <td>1.88</td>\n",
       "      <td>2.11</td>\n",
       "      <td>760.40</td>\n",
       "      <td>126</td>\n",
       "      <td>152</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5.000</td>\n",
       "      <td>63.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>445.0</td>\n",
       "      <td>1.91</td>\n",
       "      <td>1.92</td>\n",
       "      <td>737.10</td>\n",
       "      <td>121</td>\n",
       "      <td>149</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>5.240</td>\n",
       "      <td>112.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>388.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.33</td>\n",
       "      <td>906.40</td>\n",
       "      <td>122</td>\n",
       "      <td>142</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3.630</td>\n",
       "      <td>-58.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>371.0</td>\n",
       "      <td>1.81</td>\n",
       "      <td>0.48</td>\n",
       "      <td>578.80</td>\n",
       "      <td>122</td>\n",
       "      <td>136</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>4.320</td>\n",
       "      <td>28.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>298.0</td>\n",
       "      <td>1.22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>600.00</td>\n",
       "      <td>190</td>\n",
       "      <td>212</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3.183</td>\n",
       "      <td>29.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>278.0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>1.60</td>\n",
       "      <td>640.10</td>\n",
       "      <td>175</td>\n",
       "      <td>206</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.050</td>\n",
       "      <td>41.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>265.0</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.06</td>\n",
       "      <td>652.10</td>\n",
       "      <td>164</td>\n",
       "      <td>198</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4.360</td>\n",
       "      <td>86.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>251.0</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.78</td>\n",
       "      <td>684.30</td>\n",
       "      <td>154</td>\n",
       "      <td>190</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4.520</td>\n",
       "      <td>71.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>238.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.60</td>\n",
       "      <td>710.20</td>\n",
       "      <td>126</td>\n",
       "      <td>178</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4.650</td>\n",
       "      <td>101.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.98</td>\n",
       "      <td>719.70</td>\n",
       "      <td>135</td>\n",
       "      <td>173</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5.200</td>\n",
       "      <td>109.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.36</td>\n",
       "      <td>804.40</td>\n",
       "      <td>131</td>\n",
       "      <td>169</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5.470</td>\n",
       "      <td>53.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>235.0</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.76</td>\n",
       "      <td>731.00</td>\n",
       "      <td>145</td>\n",
       "      <td>165</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>4.630</td>\n",
       "      <td>125.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>217.0</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.54</td>\n",
       "      <td>708.60</td>\n",
       "      <td>139</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>4.420</td>\n",
       "      <td>107.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.34</td>\n",
       "      <td>761.00</td>\n",
       "      <td>170</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4.000</td>\n",
       "      <td>31.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>137.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.57</td>\n",
       "      <td>760.00</td>\n",
       "      <td>159</td>\n",
       "      <td>188</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4.720</td>\n",
       "      <td>14.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>130.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.95</td>\n",
       "      <td>840.00</td>\n",
       "      <td>128</td>\n",
       "      <td>185</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>5.930</td>\n",
       "      <td>106.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>131.0</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.36</td>\n",
       "      <td>880.00</td>\n",
       "      <td>137</td>\n",
       "      <td>180</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>5.760</td>\n",
       "      <td>151.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>133.0</td>\n",
       "      <td>2.28</td>\n",
       "      <td>1.49</td>\n",
       "      <td>870.00</td>\n",
       "      <td>136</td>\n",
       "      <td>177</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>5.970</td>\n",
       "      <td>205.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>129.1</td>\n",
       "      <td>2.54</td>\n",
       "      <td>0.74</td>\n",
       "      <td>890.13</td>\n",
       "      <td>136</td>\n",
       "      <td>174</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5.410</td>\n",
       "      <td>222.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>904.0</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.80</td>\n",
       "      <td>577.50</td>\n",
       "      <td>121</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>4.040</td>\n",
       "      <td>42.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>29</td>\n",
       "      <td>63.546</td>\n",
       "      <td>8.96</td>\n",
       "      <td>1357</td>\n",
       "      <td>2835</td>\n",
       "      <td>13.1</td>\n",
       "      <td>338</td>\n",
       "      <td>300</td>\n",
       "      <td>384.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>710.0</td>\n",
       "      <td>1.90</td>\n",
       "      <td>1.30</td>\n",
       "      <td>786.50</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>4.637</td>\n",
       "      <td>134.068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    M-At No.  M-At wt.  M-Density  M-M.P  M-B.P  M-Enth.fus  M-Enth.atom  \\\n",
       "0         29    63.546       8.96   1357   2835        13.1          338   \n",
       "1         29    63.546       8.96   1357   2835        13.1          338   \n",
       "2         29    63.546       8.96   1357   2835        13.1          338   \n",
       "3         29    63.546       8.96   1357   2835        13.1          338   \n",
       "4         29    63.546       8.96   1357   2835        13.1          338   \n",
       "5         29    63.546       8.96   1357   2835        13.1          338   \n",
       "6         29    63.546       8.96   1357   2835        13.1          338   \n",
       "7         29    63.546       8.96   1357   2835        13.1          338   \n",
       "8         29    63.546       8.96   1357   2835        13.1          338   \n",
       "9         29    63.546       8.96   1357   2835        13.1          338   \n",
       "10        29    63.546       8.96   1357   2835        13.1          338   \n",
       "11        29    63.546       8.96   1357   2835        13.1          338   \n",
       "12        29    63.546       8.96   1357   2835        13.1          338   \n",
       "13        29    63.546       8.96   1357   2835        13.1          338   \n",
       "14        29    63.546       8.96   1357   2835        13.1          338   \n",
       "15        29    63.546       8.96   1357   2835        13.1          338   \n",
       "16        29    63.546       8.96   1357   2835        13.1          338   \n",
       "17        29    63.546       8.96   1357   2835        13.1          338   \n",
       "18        29    63.546       8.96   1357   2835        13.1          338   \n",
       "19        29    63.546       8.96   1357   2835        13.1          338   \n",
       "20        29    63.546       8.96   1357   2835        13.1          338   \n",
       "21        29    63.546       8.96   1357   2835        13.1          338   \n",
       "22        29    63.546       8.96   1357   2835        13.1          338   \n",
       "23        29    63.546       8.96   1357   2835        13.1          338   \n",
       "24        29    63.546       8.96   1357   2835        13.1          338   \n",
       "25        29    63.546       8.96   1357   2835        13.1          338   \n",
       "\n",
       "    M-Enth.vap  M-Sp.ht Cap  M-Elec.-ve  ...  Sp.ht Cap  Elec.-ve  Surface.E  \\\n",
       "0          300        384.4         1.9  ...      567.0      1.36       1.20   \n",
       "1          300        384.4         1.9  ...      520.0      1.54       1.93   \n",
       "2          300        384.4         1.9  ...      489.0      1.63       2.38   \n",
       "3          300        384.4         1.9  ...      448.0      1.66       3.22   \n",
       "4          300        384.4         1.9  ...      479.0      1.55       3.39   \n",
       "5          300        384.4         1.9  ...      421.0      1.88       2.11   \n",
       "6          300        384.4         1.9  ...      445.0      1.91       1.92   \n",
       "7          300        384.4         1.9  ...      388.0      1.65       0.33   \n",
       "8          300        384.4         1.9  ...      371.0      1.81       0.48   \n",
       "9          300        384.4         1.9  ...      298.0      1.22       1.00   \n",
       "10         300        384.4         1.9  ...      278.0      1.33       1.60   \n",
       "11         300        384.4         1.9  ...      265.0      1.60       2.06   \n",
       "12         300        384.4         1.9  ...      251.0      2.16       2.78   \n",
       "13         300        384.4         1.9  ...      238.0      2.20       2.60   \n",
       "14         300        384.4         1.9  ...      240.0      2.28       1.98   \n",
       "15         300        384.4         1.9  ...      240.0      2.20       1.36   \n",
       "16         300        384.4         1.9  ...      235.0      1.93       0.76   \n",
       "17         300        384.4         1.9  ...      217.0      1.96       0.54   \n",
       "18         300        384.4         1.9  ...      140.0      1.50       2.34   \n",
       "19         300        384.4         1.9  ...      137.0      1.90       2.57   \n",
       "20         300        384.4         1.9  ...      130.0      2.20       2.95   \n",
       "21         300        384.4         1.9  ...      131.0      2.20       2.36   \n",
       "22         300        384.4         1.9  ...      133.0      2.28       1.49   \n",
       "23         300        384.4         1.9  ...      129.1      2.54       0.74   \n",
       "24         300        384.4         1.9  ...      904.0      1.61       0.80   \n",
       "25         300        384.4         1.9  ...      710.0      1.90       1.30   \n",
       "\n",
       "    1st Ion E  cova .radii  At.radii  Group  Period  Work F.  Elec.Aff  \n",
       "0      633.10          170       184      3       4    3.500    18.100  \n",
       "1      658.80          160       176      4       4    4.330     7.600  \n",
       "2      650.90          153       171      5       4    4.300    50.600  \n",
       "3      652.90          139       166      6       4    4.500    64.300  \n",
       "4      717.30          139       161      7       4    4.100   -50.000  \n",
       "5      760.40          126       152      9       4    5.000    63.700  \n",
       "6      737.10          121       149     10       4    5.240   112.000  \n",
       "7      906.40          122       142     12       4    3.630   -58.000  \n",
       "8      578.80          122       136     13       4    4.320    28.900  \n",
       "9      600.00          190       212      3       5    3.183    29.600  \n",
       "10     640.10          175       206      4       5    4.050    41.100  \n",
       "11     652.10          164       198      5       5    4.360    86.100  \n",
       "12     684.30          154       190      6       5    4.520    71.900  \n",
       "13     710.20          126       178      8       5    4.650   101.300  \n",
       "14     719.70          135       173      9       5    5.200   109.700  \n",
       "15     804.40          131       169     10       5    5.470    53.700  \n",
       "16     731.00          145       165     11       5    4.630   125.600  \n",
       "17     708.60          139       145     14       5    4.420   107.300  \n",
       "18     761.00          170       200      5       6    4.000    31.000  \n",
       "19     760.00          159       188      7       6    4.720    14.500  \n",
       "20     840.00          128       185      8       6    5.930   106.100  \n",
       "21     880.00          137       180      9       6    5.760   151.000  \n",
       "22     870.00          136       177     10       6    5.970   205.300  \n",
       "23     890.13          136       174     11       6    5.410   222.800  \n",
       "24     577.50          121       118     13       3    4.040    42.500  \n",
       "25     786.50          111       111     14       3    4.637   134.068  \n",
       "\n",
       "[26 rows x 36 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df_pred.drop(labels=[\"Element\",\"OH_B.E\"], axis=1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f32699b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8025214 , -0.27404806, -0.2773089 ,  0.11512286,  0.22119562,\n",
       "        0.49027085,  0.48004702,  0.2417627 ,  0.3700893 , -0.9385915 ,\n",
       "       -0.9385915 , -0.20871086, -0.05016159,  0.54401314,  0.6024143 ,\n",
       "        0.50720763,  0.5274211 ,  0.15873632, -0.7397515 ,  0.14550392,\n",
       "        0.4396949 ,  0.47202072,  0.4507174 ,  0.4100875 ,  0.22879672,\n",
       "        0.45068613], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0a6f166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.802521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.274048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.277309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.115123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.221196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.490271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.480047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.241763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.370089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.938591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.938591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.208711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.050162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.544013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.602414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.507208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.527421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.158736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.739752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.145504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.439695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.472021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.450717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.410087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.228797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.450686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "0  -0.802521\n",
       "1  -0.274048\n",
       "2  -0.277309\n",
       "3   0.115123\n",
       "4   0.221196\n",
       "5   0.490271\n",
       "6   0.480047\n",
       "7   0.241763\n",
       "8   0.370089\n",
       "9  -0.938591\n",
       "10 -0.938591\n",
       "11 -0.208711\n",
       "12 -0.050162\n",
       "13  0.544013\n",
       "14  0.602414\n",
       "15  0.507208\n",
       "16  0.527421\n",
       "17  0.158736\n",
       "18 -0.739752\n",
       "19  0.145504\n",
       "20  0.439695\n",
       "21  0.472021\n",
       "22  0.450717\n",
       "23  0.410087\n",
       "24  0.228797\n",
       "25  0.450686"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = model.predict(x)\n",
    "p = pd.DataFrame(p)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7405770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.6076237e-02 0.0000000e+00 2.3406536e-04 7.8458980e-02 0.0000000e+00\n",
      " 6.3766824e-06 0.0000000e+00 0.0000000e+00 3.3801821e-06 5.4058439e-01\n",
      " 8.4737830e-02 4.6594534e-05 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00 1.1781689e-04 1.9838860e-06 2.3844691e-03 4.6902019e-04\n",
      " 1.2801491e-03 1.8355966e-03 1.2270295e-02 2.1644393e-05 9.3786484e-03\n",
      " 4.9913633e-05 3.7441659e-04 1.3149421e-01 2.8499965e-02 6.3540884e-03\n",
      " 4.8303243e-02 5.7906628e-04 4.5648357e-03 0.0000000e+00 1.1255087e-02\n",
      " 6.1770412e-04]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model.feature_importances_)\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab02332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "from sklearn.linear_model import Lasso\n",
    "from scipy.stats.qmc import Sobol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85fed8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV , GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "params = { 'max_depth': np.arange(8,15,1),                      #10\n",
    "           'learning_rate': np.arange(0.01,0.1, 0.01),         #0.071\n",
    "           'subsample': np.arange(0.7, 1.0, 0.01),              #0.75\n",
    "           #'colsample_bytree': np.arange(0.9, 1, 0.02),         #0.82\n",
    "           'min_child_weight': np.arange(5,15, 1),\n",
    "           #'colsample_bylevel': np.arange(0.9, 1, 0.02),        #0.72\n",
    "           'n_estimators': np.arange(400,1000,25),                #116\n",
    "           #'reg_alpha': uniform(0.1,5),                     #0.8612124738904751\n",
    "           #'reg_lambda': uniform(0.1,5),                      #115.01455430429743  \n",
    "         }\n",
    "sobol_sequence = Sobol(1)\n",
    "lasso = Lasso()\n",
    "# number of times random search is run\n",
    "n = 50                                       # n=20    #n_iter=50  #cv=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc630665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 1: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.06999999999999999,\n",
      "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=13, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=400, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 1th run - Training: 0.00638409907918911, Test: 0.19170811574044683\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 2: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.060000000000000005,\n",
      "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=13, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 2th run - Training: 0.006800584762755069, Test: 0.18123796171651632\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 3: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=11, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=800, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 3th run - Training: 0.013647387742277223, Test: 0.19629660705082144\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 4: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.04, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=900, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 4th run - Training: 0.004901017437581499, Test: 0.18263589612846315\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 5: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.06999999999999999,\n",
      "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=875, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 5th run - Training: 0.0020463909838652023, Test: 0.2028464191972829\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 6: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.06999999999999999,\n",
      "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=975, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 6th run - Training: 0.001615117796095541, Test: 0.19331424147410226\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 7: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.08, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=675, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 7th run - Training: 0.0018106021365645198, Test: 0.19427749393023294\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 8: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=700, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 8th run - Training: 0.008815166475065292, Test: 0.20668095473771625\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 9: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=600, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 9th run - Training: 0.023864685123353595, Test: 0.2089576474612928\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=750, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 10th run - Training: 0.015876679257521804, Test: 0.19818257024986885\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 11: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.04, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=12, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=800, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 11th run - Training: 0.0086871644086723, Test: 0.2156992114147379\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 12: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=11, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=950, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 12th run - Training: 0.009286827832777069, Test: 0.193821724851012\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 13: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=11, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=950, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 13th run - Training: 0.007441305937105477, Test: 0.1852477823783807\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 14: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=13, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=950, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 14th run - Training: 0.004108157483506216, Test: 0.2027817873546392\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 15: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.04, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 15th run - Training: 0.016532406682518517, Test: 0.1850553444757219\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 16: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.04, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
      "             min_child_weight=7, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=850, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 16th run - Training: 0.014445389491340698, Test: 0.2120231428976274\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 17: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.02, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=675, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 17th run - Training: 0.02620850727791789, Test: 0.181905530480168\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 18: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.02, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=12, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=875, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 18th run - Training: 0.03141658523565597, Test: 0.20864843054796944\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 19: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=550, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 19th run - Training: 0.007857886341643309, Test: 0.18536625442423202\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 20: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=11, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=675, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 20th run - Training: 0.01747731566713243, Test: 0.19552581878427397\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 21: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.04, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=11, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=625, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 21th run - Training: 0.014619695264897698, Test: 0.20948793803223223\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 22: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.04, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=12, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=600, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 22th run - Training: 0.014343627631739672, Test: 0.21061744955503273\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 23: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=14, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=675, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 23th run - Training: 0.017493419963976155, Test: 0.1941683637404888\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 24: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.06999999999999999,\n",
      "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=12, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=475, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 24th run - Training: 0.009309488798440638, Test: 0.19687919843781726\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 25: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
      "             min_child_weight=7, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=725, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 25th run - Training: 0.012088717781659056, Test: 0.20524924123263855\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 26: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.06999999999999999,\n",
      "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=800, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 26th run - Training: 0.0018628148631844217, Test: 0.1893696522388123\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 27: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=575, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 27th run - Training: 0.01677843316870634, Test: 0.2129871679408329\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 28: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.08, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=14, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=550, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 28th run - Training: 0.0037498622065888906, Test: 0.19249049846516117\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 29: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.060000000000000005,\n",
      "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=725, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 29th run - Training: 0.004824213268317236, Test: 0.20429934305947003\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 30: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.09, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=11, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 30th run - Training: 0.0029305272139118765, Test: 0.21422094254469182\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 31: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.04, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=14, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=575, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 31th run - Training: 0.015724364318859672, Test: 0.21001225853836875\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 32: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.04, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=12, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=575, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 32th run - Training: 0.01750658453172645, Test: 0.21210375696171427\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 33: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.04, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=11, max_leaves=None,\n",
      "             min_child_weight=7, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=700, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 33th run - Training: 0.01651466140797575, Test: 0.21206588131927762\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 34: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.09, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=12, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=600, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 34th run - Training: 0.00237967485944438, Test: 0.20463647325936732\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 35: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=11, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=975, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 35th run - Training: 0.00745888252888137, Test: 0.20862310905203257\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 36: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.060000000000000005,\n",
      "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=11, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=450, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 36th run - Training: 0.015806283909956545, Test: 0.21861979528139766\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 37: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=13, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=650, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 37th run - Training: 0.021367496770790043, Test: 0.20800763742738318\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 38: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.08, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=825, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 38th run - Training: 0.001846894127007794, Test: 0.204592634863897\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 39: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.060000000000000005,\n",
      "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=11, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=600, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 39th run - Training: 0.00831906044309651, Test: 0.20336446982135367\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 40: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.09, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=12, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=650, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 40th run - Training: 0.0018993612034416802, Test: 0.19113590282138063\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 41: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.060000000000000005,\n",
      "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=14, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=750, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 41th run - Training: 0.005443439295380689, Test: 0.215146842038509\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 42: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=650, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 42th run - Training: 0.0056941971843281845, Test: 0.18576083622860481\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 43: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=925, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 43th run - Training: 0.00974209370781553, Test: 0.1904760623108118\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 44: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=525, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 44th run - Training: 0.03531595936132592, Test: 0.20873051202224271\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 45: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=800, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 45th run - Training: 0.013500455877858548, Test: 0.21522587591827896\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 46: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=11, max_leaves=None,\n",
      "             min_child_weight=5, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=450, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 46th run - Training: 0.017811929448402288, Test: 0.21127826068225494\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 47: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.06999999999999999,\n",
      "             max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=12, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=975, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 47th run - Training: 0.0017321291103660431, Test: 0.19506353097326923\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 48: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.03, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=13, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=475, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 48th run - Training: 0.025596109477640752, Test: 0.18670779509492674\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n",
      "Run 49: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.02, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=700, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 49th run - Training: 0.03405980505473763, Test: 0.20290888305814517\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 50: Best Estimator: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.04, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=12, max_leaves=None,\n",
      "             min_child_weight=6, missing=nan, monotone_constraints=None,\n",
      "             n_estimators=950, n_jobs=None, num_parallel_tree=None,\n",
      "             predictor=None, random_state=None, ...)\n",
      "RMSE for every 50th run - Training: 0.005870359378204717, Test: 0.19762536230818803\n",
      "Predictions from 50 best RMSE models saved to 'output_data_50_Best_xGBR.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "xgbr = xgb.XGBRegressor(seed=20)\n",
    "average = np.array([0]*36, dtype=np.float64)\n",
    "feature_importances = []\n",
    "\n",
    "nth_run = 1\n",
    "best_models = []\n",
    "rmse_values_test = []\n",
    "rmse_values_train = []\n",
    "avg = 0\n",
    "\n",
    "for i in range(n):   \n",
    "    clf = RandomizedSearchCV(estimator=xgbr,\n",
    "                             param_distributions=params,\n",
    "                             scoring='neg_mean_squared_error',\n",
    "                             n_iter=30,cv=10,random_state=np.random.RandomState(int(sobol_sequence.random(1)[0] * 2**31)),\n",
    "                             verbose=1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    \n",
    "    print(f\"Run {i + 1}: Best Estimator: {clf.best_estimator_}\")\n",
    "    best_models.append(clf.best_estimator_)\n",
    "    \n",
    "    if (i + 1) % nth_run == 0:\n",
    "        # Predictions on test set\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "        rmse_test = np.sqrt(mse_test)\n",
    "        rmse_values_test.append(rmse_test)\n",
    "        \n",
    "        # Predictions on training set\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "        rmse_train = np.sqrt(mse_train)\n",
    "        rmse_values_train.append(rmse_train)\n",
    "\n",
    "        print(f\"RMSE for every {nth_run}th run - Training: {rmse_train}, Test: {rmse_test}\")\n",
    "        nth_run += 1\n",
    "    \n",
    "    #average += clf.best_estimator_.feature_importances_\n",
    "    #feature_importances.append(clf.best_estimator_.feature_importances_)\n",
    "#average = average/n\n",
    "#avg = avg/n\n",
    "\n",
    "predictions_df = pd.DataFrame()\n",
    "\n",
    "for i, model in enumerate(best_models):\n",
    "    predictions_df[f'Run_{i+1}'] = model.predict(x)\n",
    "\n",
    "# Save predictions to an Excel file\n",
    "output_file = 'output_data_50_Best_xGBR.xlsx'\n",
    "predictions_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Predictions from 50 best RMSE models saved to '{output_file}'.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b0d618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
